{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings( 'ignore' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in libraries\n",
    "#\n",
    "import math\n",
    "import time\n",
    "import json\n",
    "import numpy    as np\n",
    "import pandas   as pd\n",
    "from   tqdm     import tqdm\n",
    "from   datetime import datetime\n",
    "\n",
    "\n",
    "# Visualization libraries\n",
    "#\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Sklearn\n",
    "#\n",
    "from sklearn               import metrics\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "# Sklearn-Optimization\n",
    "#\n",
    "import skopt\n",
    "import skopt.plots\n",
    "\n",
    "\n",
    "# hiplot\n",
    "#\n",
    "import hiplot as hip\n",
    "\n",
    "\n",
    "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "# Tensorflow library\n",
    "#\n",
    "import tensorflow \n",
    "from   tensorflow.keras.metrics                 import *\n",
    "from   tensorflow.keras.optimizers              import *\n",
    "from   tensorflow.keras.callbacks               import EarlyStopping\n",
    "from   tensorflow.keras.callbacks               import ModelCheckpoint\n",
    "from   tensorflow.keras.callbacks               import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "# NBeats library\n",
    "#\n",
    "from utils.NBeats_Keras import NBeatsNet\n",
    "\n",
    "\n",
    "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "# User libraries\n",
    "#\n",
    "from utils.PerformanceMetrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.utils.generic_utils import get_custom_objects\n",
    "from tensorflow.keras.layers import Activation\n",
    "\n",
    "def swish(x, beta = 1):\n",
    "    return (x * sigmoid(beta * x))\n",
    "\n",
    "        \n",
    "get_custom_objects().update({'swish': Activation(swish)})\n",
    "\n",
    "\n",
    "\n",
    "# from tensorflow.keras.utils  import get_custom_objects\n",
    "\n",
    "def gelu(x):\n",
    "    return 0.5 * x * (1 + tensorflow.tanh(tensorflow.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))\n",
    "    # return tf.keras.backend.hard_sigmoid(1.702 * x) * x\n",
    "\n",
    "get_custom_objects().update({'gelu':  Activation(gelu)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, os\n",
    "seed = 42\n",
    "    \n",
    "random.seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "np.random.seed(seed)\n",
    "tensorflow.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HPO parameters \n",
    "#\n",
    "n_calls         = 100\n",
    "n_random_starts =  10\n",
    "\n",
    "\n",
    "# Parameters\n",
    "#\n",
    "filename        = '../metadata/7-kanal-1.csv'\n",
    "#\n",
    "Transformation  = True\n",
    "Scaling         = 'Standard'\n",
    "#\n",
    "Lag        =  12\n",
    "Horizon    =   4\n",
    "#\n",
    "epochs     =  300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     gpus = tensorflow.config.list_physical_devices('GPU')\n",
    "#     if gpus:\n",
    "#         try:\n",
    "#             # Currently, memory growth needs to be the same across GPUs\n",
    "#             for gpu in gpus:\n",
    "#                 tensorflow.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "#             logical_gpus = tensorflow.config.list_logical_devices('GPU')\n",
    "#             print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "\n",
    "#         except RuntimeError as e:\n",
    "\n",
    "#             # Memory growth must be set before GPUs have been initialized\n",
    "#             print(e)\n",
    "# except:\n",
    "#     print('[INFO] Not GPU found')\n",
    "\n",
    "# select GPU [0, 1] --> 1 for you\n",
    "import os\n",
    "import tensorflow as tf\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"-1\"\n",
    "\n",
    "# set memory growth\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "# physical_devices = tf.config.list_physical_devices('GPU')\n",
    "# if physical_devices:\n",
    "#     tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "try:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    print('[INFO] GPU granted')\n",
    "except:\n",
    "    print('[INFO] GPU NOT granted')    \n",
    "    # Invalid device or cannot modify virtual devices once initialized.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timer\n",
    "#\n",
    "start = time.time()\n",
    "\n",
    "# Load data\n",
    "#\n",
    "df = pd.read_csv( filename )\n",
    "\n",
    "print('[INFO] Data imported')\n",
    "print('[INFO] Time: %.2f seconds' % (time.time() - start))\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Date to 'datetime64'\n",
    "#\n",
    "df['Date'] = df['Date'].astype('datetime64')\n",
    "\n",
    "# Set index\n",
    "#\n",
    "df.set_index('Date', inplace=True)\n",
    "\n",
    "\n",
    "df = df.resample('5min').mean().interpolate()\n",
    "# df = pd.DataFrame( df[['Temperature', 'Humidity', 'NO2', 'CO']] )\n",
    "\n",
    "# The last feature is the target variable\n",
    "df = pd.DataFrame( df[ ['NO2', 'CO'] ] )\n",
    "# df = pd.DataFrame( df[ 'CO' ] )\n",
    "\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetSeries = df.columns[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training/Testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = int( df.shape[0] * 0.8 )\n",
    "\n",
    "df_train = df[ :idx ]\n",
    "df_test  = df[ idx: ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure( figsize = (20, 4) );\n",
    "#\n",
    "df_train[ targetSeries ].plot()\n",
    "df_test[ targetSeries ].plot()\n",
    "#\n",
    "plt.legend(['Training', 'Testing'], fontsize = 12, frameon = False);\n",
    "plt.ylabel( targetSeries, size = 12);\n",
    "plt.xlabel( 'Time', size = 12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing Lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.concat([df_train.iloc[-Lag:], df_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (Transformation == True):\n",
    "    \n",
    "    print('[INFO] Data transformation applied')\n",
    "    \n",
    "    VALUE = max(df.min().min(), 1.0)\n",
    "    \n",
    "    df_train = np.log( df_train + VALUE)\n",
    "    df_test  = np.log( df_test  + VALUE)\n",
    "    \n",
    "else:\n",
    "    print('[INFO] No data transformation applied.')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (Scaling == 'MinMax'):\n",
    "    print('[INFO] Scaling: MinMax')\n",
    "    \n",
    "    for feature in df.columns:\n",
    "        if (feature == targetSeries): continue\n",
    "        print('Feature: ', feature)        \n",
    "        # Set scaler\n",
    "        #\n",
    "        scaler = MinMaxScaler()\n",
    "        \n",
    "        df_train[feature] = scaler.fit_transform( df_train[ feature ].to_numpy().reshape(-1,1) )\n",
    "        df_test[feature]  = scaler.transform( df_test[ feature ].to_numpy().reshape(-1,1) )\n",
    "\n",
    "        \n",
    "    # Scaling of Target Series\n",
    "    #\n",
    "    scaler = MinMaxScaler()\n",
    "    df_train[targetSeries] = scaler.fit_transform( df_train[ targetSeries ].to_numpy().reshape(-1,1) )\n",
    "    df_test[targetSeries]  = scaler.transform( df_test[ targetSeries ].to_numpy().reshape(-1,1) )\n",
    "            \n",
    "elif (Scaling == 'Robust'):\n",
    "    print('[INFO] Scaling: Robust')\n",
    "    \n",
    "    for feature in df.columns:\n",
    "        if (feature == targetSeries): continue\n",
    "        print('Feature: ', feature)        \n",
    "        # Set scaler\n",
    "        #\n",
    "        scaler = RobustScaler()\n",
    "        \n",
    "        df_train[feature] = scaler.fit_transform( df_train[ feature ].to_numpy().reshape(-1,1) )\n",
    "        df_test[feature]  = scaler.transform( df_test[ feature ].to_numpy().reshape(-1,1) )\n",
    "\n",
    "        \n",
    "    # Scaling of Target Series\n",
    "    #\n",
    "    scaler = RobustScaler()\n",
    "    df_train[targetSeries] = scaler.fit_transform( df_train[ targetSeries ].to_numpy().reshape(-1,1) )\n",
    "    df_test[targetSeries]  = scaler.transform( df_test[ targetSeries ].to_numpy().reshape(-1,1) )\n",
    "        \n",
    "elif (Scaling == 'Standard'):\n",
    "    print('[INFO] Scaling: Standard')\n",
    "\n",
    "    for feature in df.columns:\n",
    "        if (feature == targetSeries): continue\n",
    "        print('Feature: ', feature)\n",
    "        # Set scaler\n",
    "        #\n",
    "        scaler = StandardScaler()\n",
    "        \n",
    "        df_train[feature] = scaler.fit_transform( df_train[ feature ].to_numpy().reshape(-1,1) )\n",
    "        df_test[feature]  = scaler.transform( df_test[ feature ].to_numpy().reshape(-1,1) )\n",
    "\n",
    "        \n",
    "    # Scaling of Target Series\n",
    "    #\n",
    "    scaler = StandardScaler()\n",
    "    df_train[targetSeries] = scaler.fit_transform( df_train[ targetSeries ].to_numpy().reshape(-1,1) )\n",
    "    df_test[targetSeries]  = scaler.transform( df_test[ targetSeries ].to_numpy().reshape(-1,1) )\n",
    "           \n",
    "else:\n",
    "    print('[WARNING] Unknown data scaling. Standar scaling was selected')   \n",
    "    \n",
    "    for feature in df.columns:\n",
    "        if (feature == targetSeries): continue\n",
    "        print('Feature: ', feature)\n",
    "        # Set scaler\n",
    "        #\n",
    "        scaler = StandardScaler()\n",
    "        \n",
    "        df_train[feature] = scaler.fit_transform( df_train[ feature ].to_numpy().reshape(-1,1) )\n",
    "        df_test[feature]  = scaler.transform( df_test[ feature ].to_numpy().reshape(-1,1) )\n",
    "\n",
    "        \n",
    "    # Scaling of Target Series\n",
    "    #\n",
    "    scaler = StandardScaler()\n",
    "    df_train[targetSeries] = scaler.fit_transform( df_train[ targetSeries ].to_numpy().reshape(-1,1) )\n",
    "    df_test[targetSeries]  = scaler.transform( df_test[ targetSeries ].to_numpy().reshape(-1,1) )  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training/Testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(df = None, Lag = 1, Horizon = 12, targetSeries = None, includeLagData = False, includeDate = False, overlap = 1):\n",
    "    \n",
    "    if (targetSeries is None):\n",
    "        targetSeries = df.columns[-1]\n",
    "    \n",
    "    dataX, dataY = [], []\n",
    "    for i in tqdm( range(0, df.shape[0] + 1  - Lag - Horizon, overlap) ):\n",
    "        \n",
    "        # Original data\n",
    "        data       = df.to_numpy()[i+1:(i+Lag)]        \n",
    "        \n",
    "        # Include date info\n",
    "        if (includeDate):            \n",
    "            myDate = [ np.sin(df[i+1:(i+Lag)].index.hour + df[i+1:(i+Lag)].index.minute / 60 ).tolist(), \n",
    "                       np.cos(df[i+1:(i+Lag)].index.hour + df[i+1:(i+Lag)].index.minute / 60 ).tolist() ]\n",
    "            myDate = np.asarray( myDate ).T\n",
    "            #\n",
    "            # Concatenate data            \n",
    "            data = np.concatenate([data, myDate], axis = -1)\n",
    "\n",
    "        # data_diff: Lag-d differences\n",
    "        if (includeLagData):\n",
    "            data_diff  = (df.iloc[i+Lag-1] - df.iloc[i:(i+Lag-1)]).to_numpy()\n",
    "            #\n",
    "            # Concatenate data\n",
    "            data = np.concatenate([data, data_diff], axis = -1)\n",
    "                \n",
    "        \n",
    "        # Concatenate data\n",
    "        dataX.append( data )\n",
    "        \n",
    "        # Targets \n",
    "        dataY.append( df[ targetSeries ].to_numpy()[i + Lag : i + Lag + Horizon] )\n",
    "        \n",
    "        \n",
    "    return ( np.array(dataX), np.array(dataY) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, trainY = create_dataset(df             = df_train, \n",
    "                                Lag            = Lag, \n",
    "                                Horizon        = Horizon, \n",
    "                                targetSeries   = targetSeries,\n",
    "                                includeLagData = True,\n",
    "                                includeDate    = False,\n",
    "                                overlap        = 1)\n",
    "                               \n",
    "\n",
    "testX,  testY  = create_dataset(df             = df_test, \n",
    "                                Lag            = Lag, \n",
    "                                Horizon        = Horizon, \n",
    "                                targetSeries   = targetSeries,\n",
    "                                includeLagData = True,\n",
    "                                includeDate    = False,\n",
    "                                overlap        = Horizon)\n",
    "\n",
    "\n",
    "print('Training instances:   %6i' % trainX.shape[0])\n",
    "print('Testing instances:    %6i' % testX.shape[0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from   tensorflow.keras.layers                  import *\n",
    "from   tensorflow.keras.models                  import *\n",
    "\n",
    "\n",
    "def getModel( trainX, params):\n",
    "\n",
    "    # Inputs\n",
    "    #\n",
    "    Inputs = Input(shape = (trainX.shape[1], trainX.shape[2]))\n",
    "\n",
    "\n",
    "    # Encoder\n",
    "    #\n",
    "    Encoder = LSTM(units = params['units'], activation=params['activation_function'])( Inputs )\n",
    "\n",
    "    if (params['Dropout_rate'] > 0):\n",
    "        Encoder = Dropout( params['Dropout_rate'] )( Encoder )\n",
    "\n",
    "    # Decoder\n",
    "    #\n",
    "    Decoder = RepeatVector( Horizon )(Encoder)\n",
    "    #\n",
    "    Decoder = LSTM(units = params['units'], activation=params['activation_function'])( Decoder )\n",
    "    #\n",
    "    # Output\n",
    "    #\n",
    "    Outputs = Dense( Horizon, activation='linear' )( Decoder )\n",
    "\n",
    "\n",
    "    # Create model\n",
    "    #\n",
    "    model = Model(inputs = Inputs, outputs = Outputs)\n",
    "\n",
    "    return ( model )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameter_Evaluation():\n",
    "    def __init__(self, trainX, testX, trainY, testY, verbose = False):\n",
    "        # Training/Testing sets\n",
    "        #\n",
    "        self.trainX = trainX\n",
    "        self.testX  = testX\n",
    "        self.trainY = trainY\n",
    "        self.testY  = testY\n",
    "        \n",
    "        # Parameters\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        # Number of iterations\n",
    "        #\n",
    "        self.Iter   = 1\n",
    "        \n",
    "        # Best score - initialize\n",
    "        #\n",
    "        self.best_score = np.Inf\n",
    "        \n",
    "        # Hyperparameter optimization performance\n",
    "        #\n",
    "        self.hiplot_data = []\n",
    "        \n",
    "\n",
    "        \n",
    "    def evaluate_params(self, params):\n",
    "        \n",
    "        # Store parameters\n",
    "        #\n",
    "        self.hiplot_data.append(params)\n",
    "\n",
    "        \n",
    "        # Setup model\n",
    "        #\n",
    "        model = getModel(self.trainX, params)\n",
    "        \n",
    "        # Define metrics\n",
    "        #\n",
    "        metrics = [\n",
    "                    MeanAbsolutePercentageError(name = \"MAPE\", dtype = None),\n",
    "                    RootMeanSquaredError(name = 'RMSE', dtype = None),\n",
    "                  ]\n",
    "\n",
    "\n",
    "        model.compile(loss      = params['loss'], \n",
    "                      optimizer = Adam(learning_rate = params['lr']), \n",
    "                      metrics   = metrics)\n",
    "\n",
    "        # Checkpoint\n",
    "        #\n",
    "        checkpoint = ModelCheckpoint(\"checkpoints/Optimized_model_Hybrid.hdf5\", \n",
    "                                      monitor        = 'val_RMSE', \n",
    "                                      verbose        = 0, \n",
    "                                      save_best_only = True, \n",
    "                                      mode           = 'min')\n",
    "\n",
    "        # Earlystopping\n",
    "        #\n",
    "        earlystopping = EarlyStopping(monitor       = 'val_RMSE', \n",
    "                                      mode          = 'min', \n",
    "                                      verbose       = False, \n",
    "                                      patience      = 20)\n",
    "\n",
    "        # Learning rate adjustment\n",
    "        #\n",
    "        lrs_scheduler  = ReduceLROnPlateau(monitor     = 'val_RMSE', \n",
    "                                           factor      = 0.5,\n",
    "                                           patience    = 10)\n",
    "\n",
    "        # Start clock\n",
    "        #\n",
    "        start = time.time()\n",
    "        \n",
    "        # Train model\n",
    "        #\n",
    "        model.fit(self.trainX, self.trainY,        \n",
    "                  epochs           = epochs, \n",
    "                  batch_size       = params['batch_size'], \n",
    "                  callbacks        = [checkpoint, earlystopping, lrs_scheduler],\n",
    "                  verbose          = False,#self.verbose, \n",
    "                  validation_split = 0.2)\n",
    "\n",
    "        # Terminate clock\n",
    "        #\n",
    "        stop = time.time()\n",
    "        if (self.verbose):\n",
    "            print('[INFO] Model trained - Time %.2f' % (stop - start))\n",
    "\n",
    "\n",
    "        # Load the best model\n",
    "        #\n",
    "        model.load_weights('checkpoints/Optimized_model_Hybrid.hdf5')\n",
    "\n",
    "        # Evaluation\n",
    "        #\n",
    "        pred        = model.predict( self.trainX )\n",
    "        train_score = rmse(self.trainY, pred)\n",
    "        \n",
    "        pred        = model.predict( self.testX )\n",
    "        test_score  = rmse(self.testY, pred)\n",
    "        \n",
    "        # Check if test score is valid\n",
    "        if (np.isnan(test_score)): test_score = 1000\n",
    "            \n",
    "        # Export results\n",
    "        if (test_score < self.best_score):\n",
    "            print(\"Iteration: {:3.0f} - RMSE = {:6.3f}/{:6.3f} (train/test) at {} (best)\".format(self.Iter, train_score, test_score, str(datetime.now().time())[:8]))\n",
    "            self.best_score = test_score\n",
    "        else:\n",
    "            print(\"Iteration: {:3.0f} - RMSE = {:6.3f}/{:6.3f} (train/test) at {}\".format(self.Iter, train_score, test_score, str(datetime.now().time())[:8]))\n",
    "        \n",
    "        # Update Iteration counter\n",
    "        self.Iter += 1\n",
    "\n",
    "        # Store score\n",
    "        self.hiplot_data[-1]['Score'] = test_score\n",
    "        \n",
    "        return( test_score )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Parameter_Evaluation(trainX  = trainX, \n",
    "                                 testX   = testX, \n",
    "                                 trainY  = trainY, \n",
    "                                 testY   = testY, \n",
    "                                 verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = [ \n",
    "                 skopt.space.Categorical(categories = ['relu', 'elu', 'swish', 'gelu'], \n",
    "                                         name=\"activation_function\"),\n",
    "                 skopt.space.Categorical(categories = [32, 64], \n",
    "                                         name=\"batch_size\"),\n",
    "                 skopt.space.Categorical(categories = [0.0, 0.2, 0.5], \n",
    "                                         name=\"Dropout_rate\"),\n",
    "                 skopt.space.Categorical(categories = [50, 100], \n",
    "                                         name=\"units\"),\n",
    "                 skopt.space.Categorical(categories = [1e-3, 5e-4, 1e-4, 1e-5], \n",
    "                                         name=\"lr\"),\n",
    "                 skopt.space.Categorical(categories = ['mse'], \n",
    "                                         name=\"loss\"),\n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HPO_params = {\n",
    "              'n_calls':         n_calls,\n",
    "              'n_random_starts': n_random_starts,\n",
    "              'base_estimator':  'ET',\n",
    "              'acq_func':        'EI',\n",
    "             }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@skopt.utils.use_named_args( search_space )\n",
    "def objective( **params ):\n",
    "    return  evaluator.evaluate_params( params )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "results = skopt.forest_minimize(objective, search_space, **HPO_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot results - Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure( figsize = (20, 5) )\n",
    "\n",
    "skopt.plots.plot_convergence( results );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot results - Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skopt.plots.plot_evaluations(results);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skopt.plots.plot_objective(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive parallel coordinates plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_encoder(object):\n",
    "    if isinstance(object, np.generic):\n",
    "        return object.item()\n",
    "\n",
    "# Convert NumPy data types to be recognizable by json\n",
    "evaluator.hiplot_data = json.loads( json.dumps(evaluator.hiplot_data, default=np_encoder) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hip.Experiment.from_iterable( evaluator.hiplot_data ).display();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('checkpoints/Hyperparameter_optimization_Hybrid.json', 'w', encoding='utf-8') as f:\n",
    "    f.write( json.dumps( evaluator.hiplot_data ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get optimized hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_named_params(results, search_space):\n",
    "    params       = results.x\n",
    "    param_dict   = {}\n",
    "    \n",
    "    params_list  =[(dimension.name, param) for dimension, param in zip(search_space, params)]\n",
    "    \n",
    "    for item in params_list:\n",
    "        param_dict[item[0]] = item[1]\n",
    "    \n",
    "    return( param_dict )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_params = to_named_params(results, search_space)\n",
    "\n",
    "\n",
    "print('[INFO] Optimized hyperparameters\\n')\n",
    "#\n",
    "for (parameter,value) in optimized_params.items():\n",
    "    if ( isinstance(value, float) ):\n",
    "        print(' >%25s: %.5f' % (parameter,value))\n",
    "    else:\n",
    "        print(' >%25s: %s' % (parameter,value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimized (best) model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup model\n",
    "#\n",
    "model = getModel(trainX, optimized_params)\n",
    "\n",
    "\n",
    "print('[INFO] Model trained')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metrics\n",
    "#\n",
    "metrics = [\n",
    "            MeanAbsolutePercentageError(name = \"MAPE\", dtype = None),\n",
    "            RootMeanSquaredError(name = 'RMSE', dtype = None),\n",
    "          ]\n",
    "\n",
    "\n",
    "model.compile(loss      = optimized_params['loss'], \n",
    "              optimizer = Adam(learning_rate = optimized_params['lr']), \n",
    "              metrics   = metrics)\n",
    "\n",
    "\n",
    "# Checkpoint\n",
    "#\n",
    "checkpoint = ModelCheckpoint(\"checkpoints/Optimized_model_Hybrid.hdf5\", \n",
    "                              monitor        = 'val_RMSE', \n",
    "                              verbose        = 0, \n",
    "                              save_best_only = True, \n",
    "                              mode           = 'min')\n",
    "\n",
    "# Earlystopping\n",
    "#\n",
    "earlystopping = EarlyStopping(monitor       = 'val_RMSE', \n",
    "                              mode          = 'min', \n",
    "                              verbose       = 1, \n",
    "                              patience      = 20)\n",
    "\n",
    "# Learning rate adjustment\n",
    "#\n",
    "lrs_scheduler  = ReduceLROnPlateau(monitor     = 'val_RMSE', \n",
    "                                   factor      = 0.5,\n",
    "                                   patience    = 10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Start clock\n",
    "#\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "score = model.fit(trainX, trainY, \n",
    "                  epochs           = epochs, \n",
    "                  batch_size       = optimized_params['batch_size'], \n",
    "                  callbacks        = [checkpoint, earlystopping, lrs_scheduler],\n",
    "                  verbose          = True, \n",
    "                  validation_split = 0.2)\n",
    "\n",
    "\n",
    "\n",
    "# Terminate clock\n",
    "#\n",
    "stop = time.time()\n",
    "print('[INFO] Time %.2f' % (stop - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure\n",
    "#\n",
    "fig, ax = plt.subplots(nrows = 1, ncols = 2, figsize = (20, 3) )\n",
    "\n",
    "# Plot RMSE\n",
    "#\n",
    "ax[0].plot( score.history['RMSE'], marker = 'o');\n",
    "ax[0].plot( score.history['val_RMSE'], marker = 'o');\n",
    "ax[0].legend(['Training', 'Validation'], frameon = False);\n",
    "ax[0].set_title('RMSE');\n",
    "\n",
    "# Plot RMSE\n",
    "#\n",
    "ax[1].plot( score.history['MAPE'], marker = 'o');\n",
    "ax[1].plot( score.history['val_MAPE'], marker = 'o');\n",
    "ax[1].legend(['Training', 'Validation'], frameon = False);\n",
    "ax[1].set_title('MAPE');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load optimized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "#\n",
    "model.load_weights('checkpoints/Optimized_model_Hybrid.hdf5')\n",
    "\n",
    "print('[INFO] Model loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "#\n",
    "pred = model.predict( testX )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply inverse scaling/transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply inverse scaling\n",
    "#\n",
    "for i in range( Horizon ):\n",
    "    testY[:,  i] = scaler.inverse_transform( testY[:,  i].reshape(-1,1) ).squeeze(-1)\n",
    "    pred[:, i]   = scaler.inverse_transform( pred[:, i].reshape(-1,1) ).squeeze(-1)\n",
    "\n",
    "\n",
    "# Apply inverse transformation   \n",
    "#\n",
    "if (Transformation == True):\n",
    "    testY = np.exp( testY ) - VALUE\n",
    "    pred = np.exp( pred )   - VALUE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Performance on Testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('[INFO] Feature: ', targetSeries)\n",
    "print('------------------------------------------------')\n",
    "Performance_Foresting_Model = {'RMSE': [], 'MAE': [], 'SMAPE': [], 'R2' : []}\n",
    "\n",
    "for i in range( Horizon ):\n",
    "\n",
    "    Prices = pd.DataFrame([])        \n",
    "\n",
    "    Prices[targetSeries] = testY[:,i]\n",
    "    Prices['Prediction'] = pred[:,i]\n",
    "\n",
    "\n",
    "    # Evaluation\n",
    "    #\n",
    "    MAE, RMSE, MAPE, SMAPE, R2 = RegressionEvaluation( Prices )\n",
    "\n",
    "    # Store results\n",
    "    #\n",
    "    Performance_Foresting_Model['RMSE']    += [ RMSE    ]\n",
    "    Performance_Foresting_Model['MAE']     += [ MAE     ]\n",
    "    Performance_Foresting_Model['SMAPE']   += [ SMAPE   ]\n",
    "    Performance_Foresting_Model['R2']      += [ R2      ]\n",
    "\n",
    "    # Present results\n",
    "    #\n",
    "#     print('Horizon: ', i+1)\n",
    "#     print('> MAE:   ', MAE)\n",
    "#     print('> RMSE:  ', RMSE)\n",
    "#     print('> SMAPE: ', SMAPE)\n",
    "#     print('> R2:    ', R2)\n",
    "#     print()\n",
    "    \n",
    "    print('Horizon: %2i MAE %5.2f SMAPE: %5.2f R2: %.2f' % (i+1, MAE, SMAPE, R2) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reliability evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('[INFO] Feature: ', targetSeries)\n",
    "print('------------------------------------------------')\n",
    "Performance_Foresting_Model = {'RMSE': [], 'MAE': [], 'SMAPE': [], 'R2' : []}\n",
    "\n",
    "for i in range( Horizon ):\n",
    "\n",
    "    Prices = pd.DataFrame([])        \n",
    "\n",
    "    Prices[targetSeries] = testY[:,i]\n",
    "    Prices['Prediction'] = pred[:,i]\n",
    "            \n",
    "            \n",
    "\n",
    "            \n",
    "    from scipy import stats\n",
    "    from statsmodels.graphics.tsaplots import plot_acf\n",
    "    \n",
    "    res = (Prices[targetSeries] - Prices['Prediction']).to_numpy()\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows = 1, ncols = 2, figsize = (15, 2) )\n",
    "\n",
    "    ax[0].hist( res, bins = 100 )    \n",
    "    plot_acf( res, ax=ax[1] )       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Prices = pd.DataFrame( [] )\n",
    "\n",
    "Prices[targetSeries] = testY.flatten()\n",
    "Prices['Prediction'] = pred.flatten()\n",
    "\n",
    "\n",
    "# Plot results\n",
    "#\n",
    "plt.figure( figsize = (20, 4) );\n",
    "#\n",
    "Prices[ targetSeries ][:200].plot(color = 'tab:blue', marker = 'o')\n",
    "Prices[ 'Prediction' ][:200].plot(color = 'tab:orange', marker = 'o')\n",
    "plt.show();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subplots = [331, 332, 333, 334, 335, 336,  337, 338, 339]\n",
    "plt.figure( figsize = (20, 8) )\n",
    "RandomInstances = [random.randint(1, testY.shape[0]) for i in range(0, 9)]\n",
    "RandomInstances = [66, 62, 192, 448, 477, 1035, 34, 408, 860]\n",
    "\n",
    "for plot_id, i in enumerate(RandomInstances):\n",
    "\n",
    "    plt.subplot(subplots[plot_id])\n",
    "    plt.grid()\n",
    "\n",
    "    plt.plot(testY[i], color='g', marker = 'o', linewidth = 2)\n",
    "    plt.plot(pred[i],  color='r', marker = 'o', linewidth = 2)\n",
    "\n",
    "    plt.legend(['Future values', 'Prediction'], frameon = False, fontsize = 12)\n",
    "    plt.ylim([90, 260])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-dev_ili_v1] *",
   "language": "python",
   "name": "conda-env-.conda-dev_ili_v1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "7428b380d083f99fc04fcc5ca6fbebec3e7c66e9e3f92621c0f3007db81c1bd7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
